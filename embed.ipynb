{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f015154d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b32a24f37814eeab0ef6f3a8f4182c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fe47df207446558a00242d4d6d3265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3d9a92cbe2417fa09dd709e8bb72c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da1ba9323c84273b5b6d4481b20f131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6685beadbbbd4b81aae4d54630c9fdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d20cc5ad174037b93cf9759ac23a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b039e0a4558440acb9868998d5211936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc73f1296535455db600027106f5fb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd6233b14494152879c8680fe874490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677177c539a14b4d957f7b37b721a9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c4616a2ef5449eae0390bc9d709173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files to process\n",
      "Processing file 1/1: ./Shariaa-Standards-ENG.pdf\n",
      "  - Extracted 2090 nodes from file\n",
      "  - Sample content: 1388\n",
      "...\n",
      "\n",
      "Total nodes collected from all files: 2090\n",
      "\n",
      "Testing retrieval with query: 'your test query here'\n",
      "Retrieved 20 chunks\n",
      "Top result score: 0.6510479627036676\n",
      "Top result content: If the normal rent \n",
      "for the Waqf in question has risen because of developing the Waqf for the Waqf in question has risen because of developing the Waq...\n",
      "\n",
      "Saving vector database to ./vector_db_storage\n",
      "Vector database saved successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import SimpleDirectoryReader, Settings, VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Directory containing your files\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")  # Adjust device as needed\n",
    "\n",
    "\n",
    "\n",
    "files_directory = \"./data/\"  # Replace with your directory path\n",
    "\n",
    "# Get all files in the directory\n",
    "file_paths = []\n",
    "for root, _, files in os.walk(files_directory):\n",
    "    for file in files:\n",
    "        # You can add file extension filters here if needed\n",
    "        # if file.endswith('.txt') or file.endswith('.pdf'):\n",
    "        file_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(file_paths)} files to process\")\n",
    "\n",
    "\n",
    "file_paths=['./Shariaa-Standards-ENG.pdf']\n",
    "# Initialize an empty list to collect all nodes\n",
    "all_nodes = []\n",
    "\n",
    "# Configure global settings for the RAG pipeline\n",
    "Settings.embed_model = embed_model  # Using your existing embed_model\n",
    "\n",
    "# Configure document splitters\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=20, \n",
    "    breakpoint_percentile_threshold=95, \n",
    "    embed_model=embed_model\n",
    ")\n",
    "base_splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "# Process each file individually\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    try:\n",
    "        print(f\"Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "        \n",
    "        # Load a single document\n",
    "        documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "        \n",
    "        # Process document into nodes\n",
    "        nodes = splitter.get_nodes_from_documents(documents)\n",
    "        \n",
    "        print(f\"  - Extracted {len(nodes)} nodes from file\")\n",
    "        if nodes:\n",
    "            print(f\"  - Sample content: {nodes[0].get_content()[:100]}...\")\n",
    "            \n",
    "        # Add nodes from this file to the collection\n",
    "        all_nodes.extend(nodes)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - Error processing file {file_path}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal nodes collected from all files: {len(all_nodes)}\")\n",
    "\n",
    "# Create a vector index from all collected nodes\n",
    "index = VectorStoreIndex(all_nodes)\n",
    "\n",
    "# Configure a retriever with customized search parameters\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=20,  # Number of most relevant chunks to retrieve\n",
    ")\n",
    "\n",
    "# Test the vector database with a sample query\n",
    "test_query = \"your test query here\"  # Replace with your actual query\n",
    "print(f\"\\nTesting retrieval with query: '{test_query}'\")\n",
    "retrieval_results = retriever.retrieve(test_query)\n",
    "print(f\"Retrieved {len(retrieval_results)} chunks\")\n",
    "\n",
    "if retrieval_results:\n",
    "    print(f\"Top result score: {retrieval_results[0].score}\")\n",
    "    print(f\"Top result content: {retrieval_results[0].node.get_content()[:150]}...\")\n",
    "\n",
    "# Persist the vector database\n",
    "storage_path = \"./vector_db_storage\"\n",
    "print(f\"\\nSaving vector database to {storage_path}\")\n",
    "index.storage_context.persist(storage_path)\n",
    "print(\"Vector database saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
